{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LeNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LeNet5 is a small network, it contains the basic modules of deep learning: convolutional layer, pooling layer, and full link layer. It is the basis of other deep learning models. Here we analyze LeNet5 in depth. At the same time, through example analysis, deepen the understanding of the convolutional layer and pooling layer.\n",
    "\n",
    "![lenet](img/lenet-5.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Conv2D, MaxPooling2D, AveragePooling2D\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Loading Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   validation_split=0.2,\n",
    "                                   horizontal_flip = True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 70 images belonging to 10 classes.\n",
      "Found 10 images belonging to 10 classes.\n",
      "Found 80 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "training_set = train_datagen.flow_from_directory(r'C:\\Users\\roshan.gupta\\Downloads\\Family\\train',\n",
    "                                                 target_size = (64, 64),\n",
    "                                                 batch_size = 8,\n",
    "                                                 subset=\"training\",\n",
    "                                                 class_mode = 'categorical')\n",
    "\n",
    "validation_set = train_datagen.flow_from_directory(r'C:\\Users\\roshan.gupta\\Downloads\\Family\\train',\n",
    "                                                 target_size = (64, 64),\n",
    "                                                 batch_size = 8,\n",
    "                                                subset=\"validation\",\n",
    "                                                 class_mode = 'categorical')\n",
    "\n",
    "test_set = test_datagen.flow_from_directory(r'C:\\Users\\roshan.gupta\\Downloads\\Family\\test',\n",
    "                                            target_size = (64, 64),\n",
    "                                            batch_size = 8,\n",
    "                                            class_mode = 'categorical')\n",
    "\n",
    "#color_mode = \"grayscale\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "STEP_SIZE_TRAIN=training_set.n//training_set.batch_size\n",
    "STEP_SIZE_VALID=validation_set.n//validation_set.batch_size\n",
    "STEP_SIZE_TEST=test_set.n//test_set.batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Building a sequential model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Implementation with LeNet architechture - Tanh and Average Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = Sequential()\n",
    "\n",
    "# Select 6 feature convolution kernels with a size of 5 * 5 (without offset), and get 6 feature maps. The size of each feature map is 64−5 + 1 = 59 + 1 = 60\n",
    "# Parameters between input layer and C1 layer: 6 ∗ (5 ∗ 5 + 1)\n",
    "model1.add(Conv2D(6, kernel_size=(5, 5), activation='tanh', input_shape=(64, 64, 3)))\n",
    "\n",
    "# The input of this layer is the output of the first layer, which is a 60 * 60 * 6 node matrix.\n",
    "# The size of the filter used in this layer is 2 * 2, and the step length and width are both 2, so the output matrix size of this layer is 60 * 60 * 6.\n",
    "model1.add(AveragePooling2D(pool_size=(2, 2)))\n",
    "\n",
    "\n",
    "# The input matrix size of this layer is 30 * 30 * 6, the filter size used is 5 * 5, and the depth is 16. This layer does not use all 0 padding, and the step size is 1.\n",
    "# The output matrix size of this layer is 26 * 26 * 16. This layer has 5 * 5 * 6 * 16 + 16 = 2416 parameters\n",
    "model1.add(Conv2D(16, kernel_size=(5, 5), activation='tanh'))\n",
    "\n",
    "# The input matrix size of this layer is 26 * 26 * 16. The size of the filter used in this layer is 2 * 2, and the length and width steps are both 2,\n",
    "# so the output matrix size of this layer is 13 * 13 * 16.\n",
    "model1.add(AveragePooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# The input matrix size of this layer is 13 * 13 * 16. This layer is called a convolution layer in the LeNet-5 paper, but because the size of the filter is 5 * 5, #\n",
    "# So it is not different from the fully connected layer. If the nodes in the 5 * 5 * 16 matrix are pulled into a vector, then this layer is the same as the fully connected layer.\n",
    "# The number of output nodes in this layer is 120, with a total of 13 * 13 * 16 * 120 + 120 = 324600 parameters.\n",
    "model1.add(Flatten())\n",
    "\n",
    "model1.add(Dense(120, activation='tanh'))\n",
    "# The number of input nodes in this layer is 120 and the number of output nodes is 84. The total parameter is 120 * 84 + 84 = 10164 (w + b)\n",
    "\n",
    "model1.add(Dense(84, activation='tanh'))\n",
    "# The number of input nodes in this layer is 84 and the number of output nodes is 10. The total parameter is 84 * 10 + 10 = 850\n",
    "\n",
    "model1.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model1.compile(loss=keras.metrics.categorical_crossentropy, optimizer=keras.optimizers.Adam(), metrics=['accuracy'])\n",
    "#model.fit(x_train, y_train, batch_size=128, epochs=20, verbose=1, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_5 (Conv2D)            (None, 60, 60, 6)         456       \n",
      "_________________________________________________________________\n",
      "average_pooling2d_5 (Average (None, 30, 30, 6)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 26, 26, 16)        2416      \n",
      "_________________________________________________________________\n",
      "average_pooling2d_6 (Average (None, 13, 13, 16)        0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 2704)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 120)               324600    \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 84)                10164     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 10)                850       \n",
      "=================================================================\n",
      "Total params: 338,486\n",
      "Trainable params: 338,486\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/18\n",
      "Epoch 2/18\n",
      "Epoch 3/18\n",
      "Epoch 4/18\n",
      "Epoch 5/18\n",
      "Epoch 6/18\n",
      "Epoch 7/18\n",
      "Epoch 8/18\n",
      "Epoch 9/18\n",
      "Epoch 10/18\n",
      "Epoch 11/18\n",
      "Epoch 12/18\n",
      "Epoch 13/18\n",
      "Epoch 14/18\n",
      "Epoch 15/18\n",
      "Epoch 16/18\n",
      "Epoch 17/18\n",
      "Epoch 18/18\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x24a8d5bd7c8>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.fit_generator(training_set, \n",
    "                    steps_per_epoch=STEP_SIZE_TRAIN, \n",
    "                    epochs = 18, verbose=5, \n",
    "                    validation_data = validation_set, \n",
    "                    validation_steps = STEP_SIZE_VALID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 1s 69ms/step\n",
      "Test Loss: 0.920493483543396\n",
      "Test accuracy: 0.887499988079071\n"
     ]
    }
   ],
   "source": [
    "score = model1.evaluate(test_set)\n",
    "print('Test Loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.03454989 0.037281   0.00540133 0.02462806 0.04111099 0.01344202\n",
      "  0.8051072  0.00727952 0.01273685 0.01846326]]\n",
      "The predicted output is : Piyush\n"
     ]
    }
   ],
   "source": [
    "# Part 3 - Making new predictions\n",
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "test_image = image.load_img('C:/Users/roshan.gupta/Downloads/Family/Test/Piyush/2.jpg', target_size = (64, 64))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "result = model1.predict(test_image)\n",
    "training_set.class_indices\n",
    "print(result)\n",
    "res = np.argmax(result)\n",
    "\n",
    "dict1 = {0 : 'Bush', 1: 'Cats', 2: 'Dogs', 3: 'Hrithik', 4: 'Modi', 5 : 'Obama', 6 : 'Piyush', 7 : 'Roshan', \n",
    "         8: 'Salman', 9: 'Shah'}\n",
    "\n",
    "print(\"The predicted output is :\",dict1[res])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Implementation with LeNet architecture - Tanh and Max Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_9 (Conv2D)            (None, 60, 60, 6)         456       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 30, 30, 6)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 26, 26, 16)        2416      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 13, 13, 16)        0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 2704)              0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 120)               324600    \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 84)                10164     \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 10)                850       \n",
      "=================================================================\n",
      "Total params: 338,486\n",
      "Trainable params: 338,486\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2 = Sequential()\n",
    "# Select 6 feature convolution kernels with a size of 5 * 5 (without offset), and get 6 feature maps. The size of each feature map is 64−5 + 1 = 59 + 1 = 60\n",
    "# Parameters between input layer and C1 layer: 6 ∗ (5 ∗ 5 + 1)\n",
    "model2.add(Conv2D(6, kernel_size=(5, 5), activation='tanh', input_shape=(64, 64, 3)))\n",
    "\n",
    "# The input of this layer is the output of the first layer, which is a 60 * 60 * 6 node matrix.\n",
    "# The size of the filter used in this layer is 2 * 2, and the step length and width are both 2, so the output matrix size of this layer is 60 * 60 * 6.\n",
    "model2.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "\n",
    "# The input matrix size of this layer is 30 * 30 * 6, the filter size used is 5 * 5, and the depth is 16. This layer does not use all 0 padding, and the step size is 1.\n",
    "# The output matrix size of this layer is 26 * 26 * 16. This layer has 5 * 5 * 6 * 16 + 16 = 2416 parameters\n",
    "model2.add(Conv2D(16, kernel_size=(5, 5), activation='tanh'))\n",
    "\n",
    "# The input matrix size of this layer is 26 * 26 * 16. The size of the filter used in this layer is 2 * 2, and the length and width steps are both 2,\n",
    "# so the output matrix size of this layer is 13 * 13 * 16.\n",
    "model2.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# The input matrix size of this layer is 13 * 13 * 16. This layer is called a convolution layer in the LeNet-5 paper, but because the size of the filter is 5 * 5, #\n",
    "# So it is not different from the fully connected layer. If the nodes in the 5 * 5 * 16 matrix are pulled into a vector, then this layer is the same as the fully connected layer.\n",
    "# The number of output nodes in this layer is 120, with a total of 13 * 13 * 16 * 120 + 120 = 324600 parameters.\n",
    "model2.add(Flatten())\n",
    "\n",
    "model2.add(Dense(120, activation='tanh'))\n",
    "# The number of input nodes in this layer is 120 and the number of output nodes is 84. The total parameter is 120 * 84 + 84 = 10164 (w + b)\n",
    "\n",
    "model2.add(Dense(84, activation='tanh'))\n",
    "# The number of input nodes in this layer is 84 and the number of output nodes is 10. The total parameter is 84 * 10 + 10 = 850\n",
    "\n",
    "model2.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model2.compile(loss=keras.metrics.categorical_crossentropy, optimizer=keras.optimizers.Adam(), metrics=['accuracy'])\n",
    "\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/18\n",
      "Epoch 2/18\n",
      "Epoch 3/18\n",
      "Epoch 4/18\n",
      "Epoch 5/18\n",
      "Epoch 6/18\n",
      "Epoch 7/18\n",
      "Epoch 8/18\n",
      "Epoch 9/18\n",
      "Epoch 10/18\n",
      "Epoch 11/18\n",
      "Epoch 12/18\n",
      "Epoch 13/18\n",
      "Epoch 14/18\n",
      "Epoch 15/18\n",
      "Epoch 16/18\n",
      "Epoch 17/18\n",
      "Epoch 18/18\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x24a93f38188>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit_generator(training_set, \n",
    "                    steps_per_epoch=STEP_SIZE_TRAIN, \n",
    "                    epochs = 18, verbose=5, \n",
    "                    validation_data = validation_set, \n",
    "                    validation_steps = STEP_SIZE_VALID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 46ms/step\n",
      "Test Loss: 0.7357671856880188\n",
      "Test accuracy: 0.9125000238418579\n"
     ]
    }
   ],
   "source": [
    "score = model2.evaluate(test_set)\n",
    "print('Test Loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.7456319e-02 1.5266129e-02 2.5604759e-05 2.8343530e-02 7.9575914e-01\n",
      "  5.3124208e-02 2.9448282e-03 1.6396380e-03 2.5418893e-02 5.0021779e-02]]\n",
      "The predicted output is : Modi\n"
     ]
    }
   ],
   "source": [
    "# Part 3 - Making new predictions\n",
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "test_image = image.load_img('C:/Users/roshan.gupta/Downloads/Family/Test/Modi/2.jpg', target_size = (64, 64))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "result = model1.predict(test_image)\n",
    "training_set.class_indices\n",
    "print(result)\n",
    "res = np.argmax(result)\n",
    "\n",
    "dict1 = {0 : 'Bush', 1: 'Cats', 2: 'Dogs', 3: 'Hrithik', 4: 'Modi', 5 : 'Obama', 6 : 'Piyush', 7 : 'Roshan', \n",
    "         8: 'Salman', 9: 'Shah'}\n",
    "\n",
    "print(\"The predicted output is :\",dict1[res])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Implementation with LeNet architecture - Relu and Max Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_13 (Conv2D)           (None, 60, 60, 6)         456       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 30, 30, 6)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 26, 26, 16)        2416      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 13, 13, 16)        0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 2704)              0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 120)               324600    \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 84)                10164     \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 10)                850       \n",
      "=================================================================\n",
      "Total params: 338,486\n",
      "Trainable params: 338,486\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model3 = Sequential()\n",
    "# Select 6 feature convolution kernels with a size of 5 * 5 (without offset), and get 6 feature maps. The size of each feature map is 64−5 + 1 = 59 + 1 = 60\n",
    "# Parameters between input layer and C1 layer: 6 ∗ (5 ∗ 5 + 1)\n",
    "model3.add(Conv2D(6, kernel_size=(5, 5), activation='relu', input_shape=(64, 64, 3)))\n",
    "\n",
    "# The input of this layer is the output of the first layer, which is a 60 * 60 * 6 node matrix.\n",
    "# The size of the filter used in this layer is 2 * 2, and the step length and width are both 2, so the output matrix size of this layer is 60 * 60 * 6.\n",
    "model3.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "\n",
    "# The input matrix size of this layer is 30 * 30 * 6, the filter size used is 5 * 5, and the depth is 16. This layer does not use all 0 padding, and the step size is 1.\n",
    "# The output matrix size of this layer is 26 * 26 * 16. This layer has 5 * 5 * 6 * 16 + 16 = 2416 parameters\n",
    "model3.add(Conv2D(16, kernel_size=(5, 5), activation='relu'))\n",
    "\n",
    "# The input matrix size of this layer is 26 * 26 * 16. The size of the filter used in this layer is 2 * 2, and the length and width steps are both 2,\n",
    "# so the output matrix size of this layer is 13 * 13 * 16.\n",
    "model3.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# The input matrix size of this layer is 13 * 13 * 16. This layer is called a convolution layer in the LeNet-5 paper, but because the size of the filter is 5 * 5, #\n",
    "# So it is not different from the fully connected layer. If the nodes in the 5 * 5 * 16 matrix are pulled into a vector, then this layer is the same as the fully connected layer.\n",
    "# The number of output nodes in this layer is 120, with a total of 13 * 13 * 16 * 120 + 120 = 324600 parameters.\n",
    "model3.add(Flatten())\n",
    "\n",
    "model3.add(Dense(120, activation='relu'))\n",
    "# The number of input nodes in this layer is 120 and the number of output nodes is 84. The total parameter is 120 * 84 + 84 = 10164 (w + b)\n",
    "\n",
    "model3.add(Dense(84, activation='relu'))\n",
    "# The number of input nodes in this layer is 84 and the number of output nodes is 10. The total parameter is 84 * 10 + 10 = 850\n",
    "\n",
    "model3.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model3.compile(loss=keras.metrics.categorical_crossentropy, optimizer=keras.optimizers.Adam(), metrics=['accuracy'])\n",
    "\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/18\n",
      "Epoch 2/18\n",
      "Epoch 3/18\n",
      "Epoch 4/18\n",
      "Epoch 5/18\n",
      "Epoch 6/18\n",
      "Epoch 7/18\n",
      "Epoch 8/18\n",
      "Epoch 9/18\n",
      "Epoch 10/18\n",
      "Epoch 11/18\n",
      "Epoch 12/18\n",
      "Epoch 13/18\n",
      "Epoch 14/18\n",
      "Epoch 15/18\n",
      "Epoch 16/18\n",
      "Epoch 17/18\n",
      "Epoch 18/18\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x24a916e8108>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.fit_generator(training_set, \n",
    "                    steps_per_epoch=STEP_SIZE_TRAIN, \n",
    "                    epochs = 18, verbose=5, \n",
    "                    validation_data = validation_set, \n",
    "                    validation_steps = STEP_SIZE_VALID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 1s 137ms/step\n",
      "Test Loss: 2.189748764038086\n",
      "Test accuracy: 0.824999988079071\n"
     ]
    }
   ],
   "source": [
    "score = model3.evaluate(test_set)\n",
    "print('Test Loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.4552407e-02 6.1611593e-02 1.3586821e-03 7.1935385e-01 1.8925961e-02\n",
      "  1.6382116e-01 5.6543257e-03 1.9961232e-04 3.5745027e-03 9.4804092e-04]]\n",
      "The predicted output is : Hrithik\n"
     ]
    }
   ],
   "source": [
    "# Part 3 - Making new predictions\n",
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "test_image = image.load_img('C:/Users/roshan.gupta/Downloads/Family/Test/Hrithik/6.jpg', target_size = (64, 64))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "result = model1.predict(test_image)\n",
    "training_set.class_indices\n",
    "print(result)\n",
    "res = np.argmax(result)\n",
    "\n",
    "dict1 = {0 : 'Bush', 1: 'Cats', 2: 'Dogs', 3: 'Hrithik', 4: 'Modi', 5 : 'Obama', 6 : 'Piyush', 7 : 'Roshan', \n",
    "         8: 'Salman', 9: 'Shah'}\n",
    "\n",
    "print(\"The predicted output is :\",dict1[res])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Implementation with LeNet architecture - Relu and Average Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_15 (Conv2D)           (None, 60, 60, 6)         456       \n",
      "_________________________________________________________________\n",
      "average_pooling2d_7 (Average (None, 30, 30, 6)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 26, 26, 16)        2416      \n",
      "_________________________________________________________________\n",
      "average_pooling2d_8 (Average (None, 13, 13, 16)        0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 2704)              0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 120)               324600    \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 84)                10164     \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 10)                850       \n",
      "=================================================================\n",
      "Total params: 338,486\n",
      "Trainable params: 338,486\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model4 = Sequential()\n",
    "# Select 6 feature convolution kernels with a size of 5 * 5 (without offset), and get 6 feature maps. \n",
    "#The size of each feature map is 64−5 + 1 = 59 + 1 = 60\n",
    "# Parameters between input layer and C1 layer: 6 ∗ (5 ∗ 5 + 1)\n",
    "model4.add(Conv2D(6, kernel_size=(5, 5), activation='relu', input_shape=(64, 64, 3)))\n",
    "\n",
    "# The input of this layer is the output of the first layer, which is a 60 * 60 * 6 node matrix.\n",
    "# The size of the filter used in this layer is 2 * 2, and the step length and width are both 2, so the output matrix size of this layer is 60 * 60 * 6.\n",
    "model4.add(AveragePooling2D(pool_size=(2, 2)))\n",
    "\n",
    "\n",
    "# The input matrix size of this layer is 30 * 30 * 6, the filter size used is 5 * 5, and the depth is 16. This layer does not use all 0 padding, and the step size is 1.\n",
    "# The output matrix size of this layer is 26 * 26 * 16. This layer has 5 * 5 * 6 * 16 + 16 = 2416 parameters\n",
    "model4.add(Conv2D(16, kernel_size=(5, 5), activation='relu'))\n",
    "\n",
    "# The input matrix size of this layer is 26 * 26 * 16. The size of the filter used in this layer is 2 * 2, and the length and width steps are both 2,\n",
    "# so the output matrix size of this layer is 13 * 13 * 16.\n",
    "model4.add(AveragePooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# The input matrix size of this layer is 13 * 13 * 16. This layer is called a convolution layer in the LeNet-5 paper, but because the size of the filter is 5 * 5, #\n",
    "# So it is not different from the fully connected layer. If the nodes in the 5 * 5 * 16 matrix are pulled into a vector, then this layer is the same as the fully connected layer.\n",
    "# The number of output nodes in this layer is 120, with a total of 13 * 13 * 16 * 120 + 120 = 324600 parameters.\n",
    "model4.add(Flatten())\n",
    "\n",
    "model4.add(Dense(120, activation='relu'))\n",
    "# The number of input nodes in this layer is 120 and the number of output nodes is 84. The total parameter is 120 * 84 + 84 = 10164 (w + b)\n",
    "\n",
    "model4.add(Dense(84, activation='relu'))\n",
    "# The number of input nodes in this layer is 84 and the number of output nodes is 10. The total parameter is 84 * 10 + 10 = 850\n",
    "\n",
    "model4.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model4.compile(loss=keras.metrics.categorical_crossentropy, optimizer=keras.optimizers.Adam(), metrics=['accuracy'])\n",
    "\n",
    "model4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/18\n",
      "Epoch 2/18\n",
      "Epoch 3/18\n",
      "Epoch 4/18\n",
      "Epoch 5/18\n",
      "Epoch 6/18\n",
      "Epoch 7/18\n",
      "Epoch 8/18\n",
      "Epoch 9/18\n",
      "Epoch 10/18\n",
      "Epoch 11/18\n",
      "Epoch 12/18\n",
      "Epoch 13/18\n",
      "Epoch 14/18\n",
      "Epoch 15/18\n",
      "Epoch 16/18\n",
      "Epoch 17/18\n",
      "Epoch 18/18\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x24a970fa348>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model4.fit_generator(training_set, \n",
    "                    steps_per_epoch=STEP_SIZE_TRAIN, \n",
    "                    epochs = 18, verbose=5, \n",
    "                    validation_data = validation_set, \n",
    "                    validation_steps = STEP_SIZE_VALID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 1s 70ms/step\n",
      "Test Loss: 1.9003064632415771\n",
      "Test accuracy: 0.699999988079071\n"
     ]
    }
   ],
   "source": [
    "score = model4.evaluate(test_set)\n",
    "print('Test Loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.05474975 0.01296161 0.00069671 0.2829544  0.06095781 0.08145274\n",
      "  0.01436987 0.00335436 0.48727518 0.00122761]]\n",
      "The predicted output is : Salman\n"
     ]
    }
   ],
   "source": [
    "# Part 3 - Making new predictions\n",
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "test_image = image.load_img('C:/Users/roshan.gupta/Downloads/Family/Test/Salman/3.jpg', target_size = (64, 64))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "result = model1.predict(test_image)\n",
    "training_set.class_indices\n",
    "print(result)\n",
    "res = np.argmax(result)\n",
    "\n",
    "dict1 = {0 : 'Bush', 1: 'Cats', 2: 'Dogs', 3: 'Hrithik', 4: 'Modi', 5 : 'Obama', 6 : 'Piyush', 7 : 'Roshan', \n",
    "         8: 'Salman', 9: 'Shah'}\n",
    "\n",
    "print(\"The predicted output is :\",dict1[res])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3.2621877193450928, 0.16249999403953552]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model.evaluate_generator(generator=validation_set, steps=STEP_SIZE_TEST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table (Different models with different activation funcs and Poolings ):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----------------+---------------+\n",
      "| Activation Funcn |     Pooling     | Test Accuracy |\n",
      "+------------------+-----------------+---------------+\n",
      "|       Tanh       | Average Pooling |     0.8874    |\n",
      "|       Tanh       |   Max Pooling   |     0.9125    |\n",
      "|       Relu       |   Max Pooling   |     0.8249    |\n",
      "|       Relu       | Average Pooling |     0.6999    |\n",
      "+------------------+-----------------+---------------+\n"
     ]
    }
   ],
   "source": [
    "from prettytable import PrettyTable\n",
    "    \n",
    "x = PrettyTable()\n",
    "\n",
    "x.field_names = [\"Activation Funcn\", \"Pooling\", \"Test Accuracy\"]\n",
    "\n",
    "x.add_row([\"Tanh\", \"Average Pooling\", 0.8874])\n",
    "x.add_row([\"Tanh\", \"Max Pooling\" , 0.9125])\n",
    "x.add_row([\"Relu\", \"Max Pooling\" , 0.8249])\n",
    "x.add_row([\"Relu\", \"Average Pooling\" , 0.6999])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Here, we can see that architecture with activation function 'Tanh' and Pooling 'Max Pooling' yeild the best results.\n",
    "2. Architecture with 'Tanh' function giving the better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
